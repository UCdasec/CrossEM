{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "from torch import nn\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt  \n",
    "import itertools\n",
    "import random\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### handle the dataset\n",
    "class TorchDataset(Dataset):\n",
    "    def __init__(self, trs_file, label_file, trace_num, trace_offset, trace_length):\n",
    "        self.trs_file = trs_file\n",
    "        self.label_file = label_file\n",
    "        self.trace_num = trace_num\n",
    "        self.trace_offset = trace_offset\n",
    "        self.trace_length = trace_length\n",
    "        self.ToTensor = transforms.ToTensor()\n",
    "    def __getitem__(self, i):\n",
    "        index = i % self.trace_num\n",
    "        trace = self.trs_file[index,:]\n",
    "        label = self.label_file[index]\n",
    "        trace = trace[self.trace_offset:self.trace_offset+self.trace_length]\n",
    "        trace = np.reshape(trace,(1,-1))\n",
    "        trace = self.ToTensor(trace)\n",
    "        trace = np.reshape(trace, (1,-1))\n",
    "        label = torch.tensor(label, dtype=torch.long) \n",
    "        return trace.float(), label\n",
    "    def __len__(self):\n",
    "        return self.trace_num\n",
    "    \n",
    "### data loader for training\n",
    "def load_training(batch_size, kwargs):\n",
    "    data = TorchDataset(**kwargs)\n",
    "    train_loader = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=True, drop_last=True, num_workers=1, pin_memory=True)\n",
    "    return train_loader\n",
    "\n",
    "### data loader for testing\n",
    "def load_testing(batch_size, kwargs):\n",
    "    data = TorchDataset(**kwargs)\n",
    "    test_loader = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=False, drop_last=True, num_workers=1, pin_memory=True)\n",
    "    return test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arrays and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sbox = [99, 124, 119, 123, 242, 107, 111, 197, 48, 1, 103, 43, 254, 215, 171, 118, 202, 130, 201, 125, 250, 89, 71,\n",
    "        240, 173, 212, 162, 175, 156, 164, 114, 192, 183, 253, 147, 38, 54, 63, 247, 204, 52, 165, 229, 241, 113, 216,\n",
    "        49, 21, 4, 199, 35, 195, 24, 150, 5, 154, 7, 18, 128, 226, 235, 39, 178, 117, 9, 131, 44, 26, 27, 110, 90, 160,\n",
    "        82, 59, 214, 179, 41, 227, 47, 132, 83, 209, 0, 237, 32, 252, 177, 91, 106, 203, 190, 57, 74, 76, 88, 207, 208,\n",
    "        239, 170, 251, 67, 77, 51, 133, 69, 249, 2, 127, 80, 60, 159, 168, 81, 163, 64, 143, 146, 157, 56, 245, 188,\n",
    "        182, 218, 33, 16, 255, 243, 210, 205, 12, 19, 236, 95, 151, 68, 23, 196, 167, 126, 61, 100, 93, 25, 115, 96,\n",
    "        129, 79, 220, 34, 42, 144, 136, 70, 238, 184, 20, 222, 94, 11, 219, 224, 50, 58, 10, 73, 6, 36, 92, 194, 211,\n",
    "        172, 98, 145, 149, 228, 121, 231, 200, 55, 109, 141, 213, 78, 169, 108, 86, 244, 234, 101, 122, 174, 8, 186,\n",
    "        120, 37, 46, 28, 166, 180, 198, 232, 221, 116, 31, 75, 189, 139, 138, 112, 62, 181, 102, 72, 3, 246, 14, 97,\n",
    "        53, 87, 185, 134, 193, 29, 158, 225, 248, 152, 17, 105, 217, 142, 148, 155, 30, 135, 233, 206, 85, 40, 223, 140,\n",
    "        161, 137, 13, 191, 230, 66, 104, 65, 153, 45, 15, 176, 84, 187, 22]\n",
    "\n",
    "HW_byte = [0, 1, 1, 2, 1, 2, 2, 3, 1, 2, 2, 3, 2, 3, 3, 4, 1, 2, 2, 3, 2, 3, 3, 4, 2, 3, 3, 4, 3, 4, 4, 5, 1, 2, 2,\n",
    "            3, 2, 3, 3, 4, 2, 3, 3, 4, 3, 4, 4, 5, 2, 3, 3, 4, 3, 4, 4, 5, 3, 4, 4, 5, 4, 5, 5, 6, 1, 2, 2, 3, 2, 3,\n",
    "            3, 4, 2, 3, 3, 4, 3, 4, 4, 5, 2, 3, 3, 4, 3, 4, 4, 5, 3, 4, 4, 5, 4, 5, 5, 6, 2, 3, 3, 4, 3, 4, 4, 5, 3,\n",
    "            4, 4, 5, 4, 5, 5, 6, 3, 4, 4, 5, 4, 5, 5, 6, 4, 5, 5, 6, 5, 6, 6, 7, 1, 2, 2, 3, 2, 3, 3, 4, 2, 3, 3, 4,\n",
    "            3, 4, 4, 5, 2, 3, 3, 4, 3, 4, 4, 5, 3, 4, 4, 5, 4, 5, 5, 6, 2, 3, 3, 4, 3, 4, 4, 5, 3, 4, 4, 5, 4, 5, 5,\n",
    "            6, 3, 4, 4, 5, 4, 5, 5, 6, 4, 5, 5, 6, 5, 6, 6, 7, 2, 3, 3, 4, 3, 4, 4, 5, 3, 4, 4, 5, 4, 5, 5, 6, 3, 4,\n",
    "            4, 5, 4, 5, 5, 6, 4, 5, 5, 6, 5, 6, 6, 7, 3, 4, 4, 5, 4, 5, 5, 6, 4, 5, 5, 6, 5, 6, 6, 7, 4, 5, 5, 6, 5,\n",
    "            6, 6, 7, 5, 6, 6, 7, 6, 7, 7, 8]\n",
    "\n",
    "def train(epoch, model, freeze_BN=False):\n",
    "    \"\"\"\n",
    "    - epoch : the current epoch\n",
    "    - model : the current model  \n",
    "    - freeze_BN : whether to freeze batch normalization layers\n",
    "    \"\"\"\n",
    "    if freeze_BN:\n",
    "        model.eval()  # enter eval mode to freeze batch normalization layers\n",
    "    else:\n",
    "        model.train()  # enter training mode \n",
    "    # get the number of batches\n",
    "    num_iter = len(source_train_loader)\n",
    "    clf_criterion = nn.CrossEntropyLoss()\n",
    "    # train on each batch of data\n",
    "    for i, (source_data, source_label) in enumerate(source_train_loader, 1):\n",
    "        if cuda:\n",
    "            source_data, source_label = source_data.cuda(), source_label.cuda()\n",
    "        source_data, source_label = Variable(source_data), Variable(source_label)\n",
    "        optimizer.zero_grad()\n",
    "        source_preds = model(source_data)\n",
    "        preds = source_preds.data.max(1, keepdim=True)[1]\n",
    "        correct_batch = preds.eq(source_label.data.view_as(preds)).sum()\n",
    "        loss = clf_criterion(source_preds, source_label)\n",
    "        # optimize the cross-entropy loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i % log_interval == 0:\n",
    "            print('Train Epoch {}: [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tAcc: {:.6f}%'.format(\n",
    "                epoch, i * len(source_data), len(source_train_loader) * batch_size,\n",
    "                100. * i / len(source_train_loader), loss.data, 100. * correct_batch / len(source_data)))\n",
    "\n",
    "            \n",
    "### validation \n",
    "def validation(model):\n",
    "    # enter evaluation mode\n",
    "    model.eval()\n",
    "    valid_loss = 0\n",
    "    # the number of correct prediction\n",
    "    correct_valid = 0\n",
    "    clf_criterion = nn.CrossEntropyLoss()\n",
    "    for data, label in source_valid_loader:\n",
    "        if cuda:\n",
    "            data, label = data.cuda(), label.cuda()\n",
    "        data, label = Variable(data), Variable(label)\n",
    "        valid_preds = model(data)\n",
    "        # sum up batch loss\n",
    "        valid_loss += clf_criterion(valid_preds, label) \n",
    "        # get the index of the max probability\n",
    "        pred = valid_preds.data.max(1)[1] \n",
    "        # get the number of correct prediction\n",
    "        correct_valid += pred.eq(label.data.view_as(pred)).cpu().sum()\n",
    "    valid_loss /= len(source_valid_loader)\n",
    "    valid_acc = 100. * correct_valid / len(source_valid_loader.dataset)\n",
    "    print('Validation: loss: {:.4f}, accuracy: {}/{} ({:.6f}%)'.format(\n",
    "        valid_loss.data, correct_valid, len(source_valid_loader.dataset),\n",
    "        valid_acc))\n",
    "    return valid_loss, valid_acc\n",
    "\n",
    "### test/attack\n",
    "def test(model, device_id, disp_GE=True, model_flag='pretrained'):\n",
    "    \"\"\"\n",
    "    - model : the current model \n",
    "    - device_id : id of the tested device\n",
    "    - disp_GE : whether to attack/calculate guessing entropy (GE)\n",
    "    - model_flag : a string for naming GE result\n",
    "    \"\"\"\n",
    "    # enter evaluation mode\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    # the number of correct prediction\n",
    "    correct = 0\n",
    "    epoch = 0\n",
    "    clf_criterion = nn.CrossEntropyLoss()\n",
    "    if device_id == source_device_id: # attack on the source domain\n",
    "        test_num = source_test_num\n",
    "        test_loader = source_test_loader\n",
    "        real_key = real_key_01\n",
    "    else: # attack on the target domain\n",
    "        test_num = target_test_num\n",
    "        test_loader = target_test_loader\n",
    "        real_key = real_key_02\n",
    "    # Initialize the prediction and label lists(tensors)\n",
    "    predlist=torch.zeros(0,dtype=torch.long, device='cpu')\n",
    "    lbllist=torch.zeros(0,dtype=torch.long, device='cpu')\n",
    "    test_preds_all = torch.zeros((test_num, class_num), dtype=torch.float, device='cpu')\n",
    "    for data, label in test_loader:\n",
    "        if cuda:\n",
    "            data, label = data.cuda(), label.cuda()\n",
    "        data, label = Variable(data), Variable(label)\n",
    "        test_preds = model(data)\n",
    "        # sum up batch loss\n",
    "        test_loss += clf_criterion(test_preds, label) \n",
    "        # get the index of the max probability\n",
    "        pred = test_preds.data.max(1)[1]\n",
    "        # get the softmax results for attack/showing guessing entropy\n",
    "        softmax = nn.Softmax(dim=1)\n",
    "        test_preds_all[epoch*batch_size:(epoch+1)*batch_size, :] =softmax(test_preds)\n",
    "        # get the predictions (predlist) and real labels (lbllist) for showing confusion matrix\n",
    "        predlist=torch.cat([predlist,pred.view(-1).cpu()])\n",
    "        lbllist=torch.cat([lbllist,label.view(-1).cpu()])\n",
    "        # get the number of correct prediction\n",
    "        correct += pred.eq(label.data.view_as(pred)).cpu().sum()\n",
    "        epoch += 1\n",
    "    test_loss /= len(test_loader)\n",
    "    print('Target test loss: {:.4f}, Target test accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "        test_loss.data, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    # get the confusion matrix\n",
    "    confusion_mat = confusion_matrix(lbllist.numpy(), predlist.numpy())\n",
    "    # show the confusion matrix\n",
    "    plot_sonfusion_matrix(confusion_mat, classes = range(class_num))\n",
    "    # show the guessing entropy and success rate\n",
    "    if disp_GE:\n",
    "        plot_guessing_entropy(test_preds_all.numpy(), real_key, device_id, model_flag)\n",
    "def CDP_train(epoch, model, source_train_loader, target_finetune_loader):\n",
    "    \"\"\"\n",
    "    - epoch : the current epoch\n",
    "    - model : the current model\n",
    "    - source_train_loader: DataLoader for the source domain\n",
    "    - target_finetune_loader: DataLoader for the target domain\n",
    "    \"\"\"\n",
    "    # enter evaluation mode to freeze the BN and dropout (if have) layer when fine-tuning\n",
    "    model.eval()\n",
    "    clf_criterion = nn.CrossEntropyLoss()\n",
    "    # train on each batch of data\n",
    "    for i, (source_data, source_label) in enumerate(source_train_loader):\n",
    "        # get traces for target domain\n",
    "        target_data, _ = next(iter(target_finetune_loader))\n",
    "        if cuda:\n",
    "            source_data, source_label = source_data.cuda(), source_label.cuda()\n",
    "            target_data = target_data.cuda()\n",
    "        source_data, source_label = Variable(source_data), Variable(source_label)\n",
    "        target_data = Variable(target_data)\n",
    "        optimizer.zero_grad()\n",
    "        # get predictions and MMD loss\n",
    "        source_preds, mmd_loss = model(source_data, target_data)\n",
    "        preds = source_preds.data.max(1, keepdim=True)[1]\n",
    "        # get classification loss on source domain\n",
    "        clf_loss = clf_criterion(source_preds, source_label)\n",
    "        # the total loss function\n",
    "        loss = clf_loss + lambda_ * mmd_loss\n",
    "        # optimize the total loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i % log_interval == 0:\n",
    "            print('Train Epoch {}: [{}/{} ({:.0f}%)]\\ttotal_loss: {:.6f}\\tclf_loss: {:.6f}\\tmmd_loss: {:.6f}'.format(\n",
    "                epoch, i * len(source_data), len(source_train_loader) * batch_size,\n",
    "                100. * i / len(source_train_loader), loss.data, clf_loss.data, mmd_loss.data))\n",
    "\n",
    "\n",
    "def CDP_validation(model, source_valid_loader, target_finetune_loader):\n",
    "    # enter evaluation mode\n",
    "    clf_criterion = nn.CrossEntropyLoss()\n",
    "    model.eval()\n",
    "    # the classification loss\n",
    "    total_clf_loss = 0\n",
    "    # the MMD loss\n",
    "    total_mmd_loss = 0\n",
    "    # the total loss\n",
    "    total_loss = 0\n",
    "    # the number of correct predictions\n",
    "    correct = 0\n",
    "    \n",
    "    for i, (source_data, source_label) in enumerate(source_valid_loader):\n",
    "        # get traces for target domain\n",
    "        target_data, _ = next(iter(target_finetune_loader))\n",
    "        if cuda:\n",
    "            source_data, source_label = source_data.cuda(), source_label.cuda()\n",
    "            target_data = target_data.cuda()\n",
    "        source_data, source_label = Variable(source_data), Variable(source_label)\n",
    "        target_data = Variable(target_data)\n",
    "        \n",
    "        valid_preds, mmd_loss = model(source_data, target_data)\n",
    "        clf_loss = clf_criterion(valid_preds, source_label) \n",
    "        \n",
    "        # sum up batch loss\n",
    "        loss = clf_loss + lambda_ * mmd_loss\n",
    "        total_clf_loss += clf_loss\n",
    "        total_mmd_loss += mmd_loss\n",
    "        total_loss += loss\n",
    "        \n",
    "        # get the index of the max probability\n",
    "        pred = valid_preds.data.max(1)[1] \n",
    "        correct += pred.eq(source_label.data.view_as(pred)).cpu().sum()\n",
    "    \n",
    "    total_loss /= len(source_valid_loader)\n",
    "    total_clf_loss /= len(source_valid_loader)\n",
    "    total_mmd_loss /= len(source_valid_loader)\n",
    "    \n",
    "    print('Validation: total_loss: {:.4f}, clf_loss: {:.4f}, mmd_loss: {:.4f}, accuracy: {}/{} ({:.2f}%)'.format(\n",
    "        total_loss.data, total_clf_loss, total_mmd_loss, correct, len(source_valid_loader.dataset),\n",
    "        100. * correct / len(source_valid_loader.dataset)))\n",
    "    \n",
    "    return total_loss\n",
    "\n",
    "### kernel function\n",
    "def guassian_kernel(source, target, kernel_mul=2.0, kernel_num=5, fix_sigma=None):\n",
    "    \"\"\"\n",
    "    - source : source data\n",
    "    - target : target data\n",
    "    - kernel_mul : multiplicative step of bandwidth (sigma)\n",
    "    - kernel_num : the number of guassian kernels\n",
    "    - fix_sigma : use a fix value of bandwidth\n",
    "    \"\"\"\n",
    "    n_samples = int(source.size()[0])+int(target.size()[0])\n",
    "    total = torch.cat([source, target], dim=0)\n",
    "    total0 = total.unsqueeze(0).expand(int(total.size(0)), \\\n",
    "                                       int(total.size(0)), \\\n",
    "                                       int(total.size(1)))\n",
    "    total1 = total.unsqueeze(1).expand(int(total.size(0)), \\\n",
    "                                       int(total.size(0)), \\\n",
    "                                       int(total.size(1)))\n",
    "    # |x-y|\n",
    "    L2_distance = ((total0-total1)**2).sum(2) \n",
    "    \n",
    "    # bandwidth\n",
    "    if fix_sigma:\n",
    "        bandwidth = fix_sigma\n",
    "    else:\n",
    "        bandwidth = torch.sum(L2_distance.data) / (n_samples**2-n_samples)\n",
    "    # take the current bandwidth as the median value, and get a list of bandwidths (for example, when bandwidth is 1, we get [0.25,0.5,1,2,4]). \n",
    "    bandwidth /= kernel_mul ** (kernel_num // 2)\n",
    "    bandwidth_list = [bandwidth * (kernel_mul**i) for i in range(kernel_num)]\n",
    "\n",
    "    # exp(-|x-y|/bandwidth)\n",
    "    kernel_val = [torch.exp(-L2_distance / bandwidth_temp) for \\\n",
    "                  bandwidth_temp in bandwidth_list]\n",
    "\n",
    "    # return the final kernel matrix\n",
    "    return sum(kernel_val)\n",
    "\n",
    "### MMD loss function based on guassian kernels\n",
    "def mmd_rbf(source, target, kernel_mul=2.0, kernel_num=5, fix_sigma=None):\n",
    "    \"\"\"\n",
    "    - source : source data\n",
    "    - target : target data\n",
    "    - kernel_mul : multiplicative step of bandwidth (sigma)\n",
    "    - kernel_num : the number of guassian kernels\n",
    "    - fix_sigma : use a fix value of bandwidth\n",
    "    \"\"\"\n",
    "    loss = 0.0\n",
    "    batch_size = int(source.size()[0])\n",
    "    kernels = guassian_kernel(source, target,kernel_mul=kernel_mul,kernel_num=kernel_num, fix_sigma=fix_sigma)\n",
    "    XX = kernels[:batch_size, :batch_size] # Source<->Source\n",
    "    YY = kernels[batch_size:, batch_size:] # Target<->Target\n",
    "    XY = kernels[:batch_size, batch_size:] # Source<->Target\n",
    "    YX = kernels[batch_size:, :batch_size] # Target<->Source\n",
    "    loss = torch.mean(XX + YY - XY -YX)\n",
    "    return loss\n",
    "\n",
    "### show the guessing entropy and success rate\n",
    "def plot_guessing_entropy(preds, real_key, device_id, model_flag):\n",
    "    \"\"\"\n",
    "    - preds : the probability for each class (n*256 for a byte, n*9 for Hamming weight)\n",
    "    - real_key : the key of the target device\n",
    "    - device_id : id of the target device\n",
    "    - model_flag : a string for naming GE result\n",
    "    \"\"\"\n",
    "    # GE/SR is averaged over 100 attacks \n",
    "    num_averaged = 100\n",
    "    # max trace num for attack\n",
    "    trace_num_max = 500\n",
    "    guessing_entropy = np.zeros((num_averaged, trace_num_max))\n",
    "    success_flag = np.zeros((num_averaged, trace_num_max))\n",
    "    if device_id == target_device_id: # attack on the target domain\n",
    "        plaintext = plaintexts_target\n",
    "    elif device_id == source_device_id: # attack on the source domain\n",
    "        plaintext = plaintexts_source\n",
    "    # attack multiples times for average\n",
    "    for time in range(num_averaged):\n",
    "        # select the attack traces randomly\n",
    "        random_index = list(range(plaintext.shape[0]))\n",
    "        random.shuffle(random_index)\n",
    "        random_index = random_index[0:trace_num_max]\n",
    "        # initialize score matrix\n",
    "        score_mat = np.zeros((trace_num_max, 256))\n",
    "        for key_guess in range(0, 256):\n",
    "            for i in range(0, trace_num_max):\n",
    "                initialState = plaintext[random_index[i]] ^ key_guess\n",
    "                sout = Sbox[initialState]\n",
    "                if labeling_method == 'identity':\n",
    "                    label = sout\n",
    "                elif labeling_method == 'hw':\n",
    "                    label = HW_byte[sout]\n",
    "                score_mat[i, key_guess] = preds[random_index[i], label]\n",
    "        score_mat = np.log(score_mat + 1e-40)\n",
    "        for i in range(0, trace_num_max):\n",
    "            log_likelihood = np.sum(score_mat[0:i+1,:], axis=0)\n",
    "            ranked = np.argsort(log_likelihood)[::-1]\n",
    "            guessing_entropy[time,i] =  list(ranked).index(real_key)\n",
    "            if list(ranked).index(real_key) == 0:\n",
    "                    success_flag[time, i] = 1\n",
    "    guessing_entropy = np.mean(guessing_entropy,axis=0)\n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    p1, = plt.plot(guessing_entropy[0:trace_num_max],color='red')\n",
    "    plt.xlabel('Number of trace')\n",
    "    plt.ylabel('Guessing entropy')\n",
    "    #np.save('./results/entropy_'+ labeling_method + '_{}_to_{}_'.format(source_device_id, device_id) + model_flag, guessing_entropy)\n",
    "    plt.subplot(1, 2, 2)       \n",
    "    success_flag = np.sum(success_flag, axis=0)\n",
    "    success_rate = success_flag/num_averaged \n",
    "    p2, = plt.plot(success_rate[0:trace_num_max], color='red')\n",
    "    plt.xlabel('Number of trace')\n",
    "    plt.ylabel('Success rate')\n",
    "    plt.show()\n",
    "    #np.save('./results/success_rate_' + labeling_method + '_{}_to_{}_'.format(source_device_id, device_id) + model_flag, success_rate)\n",
    "\n",
    "### show the confusion matrix \n",
    "def plot_sonfusion_matrix(cm, classes, normalize=False, title='Confusion matrix',cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    if normalize:\n",
    "        cm = cm.astype('float')/cm.sum(axis=1)[:,np.newaxis]\n",
    "    thresh = cm.max()/2.0\n",
    "    for i,j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j,i,cm[i,j], horizontalalignment='center',color='white' if cm[i,j] > thresh else 'black')   \n",
    "    plt.ylim((len(classes)-0.5, -0.5))\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predict label')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data complete!\n"
     ]
    }
   ],
   "source": [
    "source_device_id = 1\n",
    "target_device_id = 2\n",
    "real_key_01 = 0x15 # key of the source domain\n",
    "real_key_02 = 0xd8 # key of the target domain\n",
    "lambda_ = 0.1 # Penalty coefficient\n",
    "labeling_method = 'hw' # labeling of trace\n",
    "preprocess = 'horizontal_standardization' # preprocess method\n",
    "batch_size = 250\n",
    "total_epoch = 100\n",
    "finetune_epoch = 15 # epoch number for fine-tuning\n",
    "lr = 0.001 # learning rate\n",
    "log_interval = 20 # epoch interval to log training information\n",
    "train_num = 20000\n",
    "valid_num = 1000\n",
    "source_test_num = 5000\n",
    "target_finetune_num = 5000\n",
    "target_test_num = 1\n",
    "trace_offset = 0\n",
    "trace_length = 1500\n",
    "source_file_path = './Data/ourdata/xmega/RD/T1/'\n",
    "target_file_path = './Data/ourdata/xmega/RD/L12/'\n",
    "no_cuda =False\n",
    "cuda = not no_cuda and torch.cuda.is_available()\n",
    "seed = 8\n",
    "torch.manual_seed(seed)\n",
    "if cuda:\n",
    "    torch.cuda.manual_seed(seed)\n",
    "if labeling_method == 'identity':\n",
    "    class_num = 256\n",
    "elif labeling_method == 'hw':\n",
    "    class_num = 9\n",
    "    \n",
    "# to load traces and labels\n",
    "X_train_source = np.load(source_file_path + 'X_train.npy')\n",
    "Y_train_source = np.load(source_file_path + 'Y_train.npy')\n",
    "X_attack_source = np.load(source_file_path + 'X_attack.npy')\n",
    "Y_attack_source = np.load(source_file_path + 'Y_attack.npy')\n",
    "X_attack_target = np.load(target_file_path + 'X_attack.npy')\n",
    "Y_attack_target = np.load(target_file_path + 'Y_attack.npy')\n",
    "\n",
    "\n",
    "\n",
    "X_attack_source =X_attack_source[0:5000]\n",
    "Y_attack_source =Y_attack_source[0:5000]\n",
    "X_attack_target =X_attack_target[0:5000]\n",
    "Y_attack_target =Y_attack_target[0:5000]\n",
    "\n",
    "\n",
    "# to load plaintexts\n",
    "plaintexts_source = np.load(source_file_path + 'plaintexts_attack.npy')\n",
    "plaintexts_target = np.load(target_file_path + 'plaintexts_attack.npy')\n",
    "plaintexts_target = plaintexts_target[target_finetune_num:target_finetune_num+target_test_num]\n",
    "\n",
    "# preprocess of traces\n",
    "if preprocess == 'horizontal_standardization':\n",
    "    mn = np.repeat(np.mean(X_train_source, axis=1, keepdims=True), X_train_source.shape[1], axis=1)\n",
    "    std = np.repeat(np.std(X_train_source, axis=1, keepdims=True), X_train_source.shape[1], axis=1)\n",
    "    X_train_source = (X_train_source - mn)/std\n",
    "\n",
    "    mn = np.repeat(np.mean(X_attack_source, axis=1, keepdims=True), X_attack_source.shape[1], axis=1)\n",
    "    std = np.repeat(np.std(X_attack_source, axis=1, keepdims=True), X_attack_source.shape[1], axis=1)\n",
    "    X_attack_source = (X_attack_source - mn)/std\n",
    "    \n",
    "    mn = np.repeat(np.mean(X_attack_target, axis=1, keepdims=True), X_attack_target.shape[1], axis=1)\n",
    "    std = np.repeat(np.std(X_attack_target, axis=1, keepdims=True), X_attack_target.shape[1], axis=1)\n",
    "    X_attack_target = (X_attack_target - mn)/std  \n",
    "elif preprocess == 'horizontal_scaling':\n",
    "    scaler = preprocessing.MinMaxScaler(feature_range=(-1, 1)).fit(X_train_source.T)\n",
    "    X_train_source = scaler.transform(X_train_source.T).T\n",
    "\n",
    "    scaler = preprocessing.MinMaxScaler(feature_range=(-1, 1)).fit(X_attack_source.T)\n",
    "    X_attack_source = scaler.transform(X_attack_source.T).T\n",
    "    \n",
    "    scaler = preprocessing.MinMaxScaler(feature_range=(-1, 1)).fit(X_attack_target.T)\n",
    "    X_attack_target = scaler.transform(X_attack_target.T).T\n",
    "\n",
    "# parameters of data loader\n",
    "kwargs_source_train = {\n",
    "        'trs_file': X_train_source[0:train_num,:],\n",
    "        'label_file': Y_train_source[0:train_num],\n",
    "        'trace_num':train_num,\n",
    "        'trace_offset':trace_offset,\n",
    "        'trace_length':trace_length,\n",
    "}\n",
    "kwargs_source_valid = {\n",
    "        'trs_file': X_train_source[train_num:train_num+valid_num,:],\n",
    "        'label_file': Y_train_source[train_num:train_num+valid_num],\n",
    "        'trace_num':valid_num,\n",
    "        'trace_offset':trace_offset,\n",
    "        'trace_length':trace_length,\n",
    "}\n",
    "kwargs_source_test = {\n",
    "        'trs_file': X_attack_source,\n",
    "        'label_file': Y_attack_source,\n",
    "        'trace_num':source_test_num,\n",
    "        'trace_offset':trace_offset,\n",
    "        'trace_length':trace_length,\n",
    "}\n",
    "kwargs_target_finetune = {\n",
    "        'trs_file': X_attack_target[0:target_finetune_num,:],\n",
    "        'label_file': Y_attack_target[0:target_finetune_num],\n",
    "        'trace_num':target_finetune_num,\n",
    "        'trace_offset':trace_offset,\n",
    "        'trace_length':trace_length,\n",
    "}\n",
    "kwargs_target = {\n",
    "        'trs_file': X_attack_target[target_finetune_num:target_finetune_num+target_test_num, :],\n",
    "        'label_file': Y_attack_target[target_finetune_num:target_finetune_num+target_test_num],\n",
    "        'trace_num':target_test_num,\n",
    "        'trace_offset':trace_offset,\n",
    "        'trace_length':trace_length,\n",
    "}\n",
    "source_train_loader = load_training(batch_size, kwargs_source_train)\n",
    "source_valid_loader = load_training(batch_size, kwargs_source_valid)\n",
    "source_test_loader = load_testing(batch_size, kwargs_source_test)\n",
    "target_finetune_loader = load_training(batch_size, kwargs_target_finetune)\n",
    "target_test_loader = load_testing(batch_size, kwargs_target)\n",
    "print('Load data complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 1500)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(X_attack_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "### the pre-trained model\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, num_classes=class_num):\n",
    "        super(Net, self).__init__()\n",
    "        # the encoder part\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv1d(1, 16, kernel_size=1),\n",
    "            nn.SELU(),\n",
    "            nn.AvgPool1d(kernel_size=2, stride=2),\n",
    "            nn.Conv1d(16, 32, kernel_size=50),\n",
    "            nn.SELU(),\n",
    "            nn.AvgPool1d(kernel_size=50, stride=50),\n",
    "            nn.Conv1d(32, 64, kernel_size=3),\n",
    "            nn.SELU(),\n",
    "            nn.AvgPool1d(kernel_size=2, stride=2),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        # the fully-connected layer 1\n",
    "        self.classifier_1 = nn.Sequential(\n",
    "            nn.Linear(384, 20),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        # the output layer\n",
    "        self.final_classifier = nn.Sequential(\n",
    "            nn.Linear(20, num_classes)\n",
    "        )\n",
    "\n",
    "    # how the network runs\n",
    "    def forward(self, input):\n",
    "        x = self.features(input)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier_1(x)\n",
    "        output = self.final_classifier(x)\n",
    "        return output\n",
    "\n",
    "### the fine-tuning model\n",
    "class CDP_Net(nn.Module):\n",
    "    def __init__(self, num_classes=class_num):\n",
    "        super(CDP_Net, self).__init__()\n",
    "        # the encoder part\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv1d(1, 16, kernel_size=1),\n",
    "            nn.SELU(),\n",
    "            nn.AvgPool1d(kernel_size=2, stride=2),\n",
    "            nn.Conv1d(16, 32, kernel_size=50),\n",
    "            nn.SELU(),\n",
    "            nn.AvgPool1d(kernel_size=50, stride=50),\n",
    "            nn.Conv1d(32, 64, kernel_size=3),\n",
    "            nn.SELU(),\n",
    "            nn.AvgPool1d(kernel_size=2, stride=2),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        # the fully-connected layer 1\n",
    "        self.classifier_1 = nn.Sequential(\n",
    "            nn.Linear(384, 20),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        # the output layer\n",
    "        self.final_classifier = nn.Sequential(\n",
    "            nn.Linear(20, num_classes)\n",
    "        )\n",
    "        \n",
    "    # how the network runs\n",
    "    def forward(self, source, target):\n",
    "        mmd_loss = 0\n",
    "        #source data flow\n",
    "        source = self.features(source)\n",
    "        source_0 = source.view(source.size(0), -1)\n",
    "        source_1 = self.classifier_1(source_0)\n",
    "        \n",
    "        #target data flow\n",
    "        target = self.features(target)\n",
    "        target = target.view(target.size(0), -1)\n",
    "        mmd_loss += mmd_rbf(source_0, target)\n",
    "        target = self.classifier_1(target)\n",
    "        mmd_loss += mmd_rbf(source_1, target)\n",
    "        \n",
    "        result = self.final_classifier(source_1)\n",
    "        return result, mmd_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Device Profiling: fine-tune 15 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Construct model complete\n",
      "Train Epoch 1:\n",
      "Train Epoch 1: [0/20000 (0%)]\ttotal_loss: 2.025584\tclf_loss: 1.038786\tmmd_loss: 9.867977\n",
      "Train Epoch 1: [5000/20000 (25%)]\ttotal_loss: 1.263683\tclf_loss: 1.060992\tmmd_loss: 2.026912\n",
      "Train Epoch 1: [10000/20000 (50%)]\ttotal_loss: 1.228037\tclf_loss: 1.117540\tmmd_loss: 1.104965\n",
      "Train Epoch 1: [15000/20000 (75%)]\ttotal_loss: 1.257194\tclf_loss: 1.171754\tmmd_loss: 0.854406\n",
      "Validation: total_loss: 1.1564, clf_loss: 1.0799, mmd_loss: 0.7648, accuracy: 530/1000 (53.00%)\n",
      "Train Epoch 2:\n",
      "Train Epoch 2: [0/20000 (0%)]\ttotal_loss: 1.182548\tclf_loss: 1.112047\tmmd_loss: 0.705011\n",
      "Train Epoch 2: [5000/20000 (25%)]\ttotal_loss: 1.332545\tclf_loss: 1.261275\tmmd_loss: 0.712697\n"
     ]
    }
   ],
   "source": [
    "# create a network\n",
    "CDP_model = CDP_Net(num_classes=class_num)\n",
    "print('Construct model complete')\n",
    "if cuda:\n",
    "    CDP_model.cuda()\n",
    "# initialize a big enough loss\n",
    "min_loss = 1000\n",
    "# load the pre-trained model\n",
    "checkpoint = torch.load('./models/xmega/RD/T1_L11L11.pth'.format(source_device_id))\n",
    "pretrained_dict = checkpoint['model_state_dict']\n",
    "CDP_model.load_state_dict(pretrained_dict)\n",
    "optimizer = optim.Adam([\n",
    "        {'params': CDP_model.features.parameters()},\n",
    "        {'params': CDP_model.classifier_1.parameters()},\n",
    "        {'params': CDP_model.final_classifier.parameters()}\n",
    "    ], lr=lr)\n",
    "# restore the optimizer state\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "# Usage:\n",
    "for epoch in range(1, 150 + 1):\n",
    "    print(f'Train Epoch {epoch}:')\n",
    "    CDP_train(epoch, CDP_model, source_train_loader, target_finetune_loader)\n",
    "    with torch.no_grad():\n",
    "        valid_loss = CDP_validation(CDP_model, source_valid_loader, target_finetune_loader)\n",
    "        if valid_loss < min_loss:\n",
    "            min_loss = valid_loss\n",
    "            torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': CDP_model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict()\n",
    "            }, './models/xmega/RD/mutlirunner/last_valid_loss_finetuned_deviceL11L12_5000.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
