{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mabon_pytorch/anaconda3/envs/gpu/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "from torch import nn\n",
    "import torchvision\n",
    "\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt  \n",
    "import itertools\n",
    "import random\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### handle the dataset\n",
    "class TorchDataset(Dataset):\n",
    "    def __init__(self, trs_file, label_file, trace_num, trace_offset, trace_length):\n",
    "        self.trs_file = trs_file\n",
    "        self.label_file = label_file\n",
    "        self.trace_num = trace_num\n",
    "        self.trace_offset = trace_offset\n",
    "        self.trace_length = trace_length\n",
    "        self.ToTensor = transforms.ToTensor()\n",
    "    def __getitem__(self, i):\n",
    "        index = i % self.trace_num\n",
    "        trace = self.trs_file[index,:]\n",
    "        label = self.label_file[index]\n",
    "        trace = trace[self.trace_offset:self.trace_offset+self.trace_length]\n",
    "        trace = np.reshape(trace,(1,-1))\n",
    "        trace = self.ToTensor(trace)\n",
    "        trace = np.reshape(trace, (1,-1))\n",
    "        label = torch.tensor(label, dtype=torch.long) \n",
    "        return trace.float(), label\n",
    "    def __len__(self):\n",
    "        return self.trace_num\n",
    "    \n",
    "### data loader for training\n",
    "def load_training(batch_size, kwargs):\n",
    "    data = TorchDataset(**kwargs)\n",
    "    train_loader = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=True, drop_last=True, num_workers=1, pin_memory=True)\n",
    "    return train_loader\n",
    "\n",
    "### data loader for testing\n",
    "def load_testing(batch_size, kwargs):\n",
    "    data = TorchDataset(**kwargs)\n",
    "    test_loader = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=False, drop_last=True, num_workers=1, pin_memory=True)\n",
    "    return test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arrays and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sbox = [99, 124, 119, 123, 242, 107, 111, 197, 48, 1, 103, 43, 254, 215, 171, 118, 202, 130, 201, 125, 250, 89, 71,\n",
    "        240, 173, 212, 162, 175, 156, 164, 114, 192, 183, 253, 147, 38, 54, 63, 247, 204, 52, 165, 229, 241, 113, 216,\n",
    "        49, 21, 4, 199, 35, 195, 24, 150, 5, 154, 7, 18, 128, 226, 235, 39, 178, 117, 9, 131, 44, 26, 27, 110, 90, 160,\n",
    "        82, 59, 214, 179, 41, 227, 47, 132, 83, 209, 0, 237, 32, 252, 177, 91, 106, 203, 190, 57, 74, 76, 88, 207, 208,\n",
    "        239, 170, 251, 67, 77, 51, 133, 69, 249, 2, 127, 80, 60, 159, 168, 81, 163, 64, 143, 146, 157, 56, 245, 188,\n",
    "        182, 218, 33, 16, 255, 243, 210, 205, 12, 19, 236, 95, 151, 68, 23, 196, 167, 126, 61, 100, 93, 25, 115, 96,\n",
    "        129, 79, 220, 34, 42, 144, 136, 70, 238, 184, 20, 222, 94, 11, 219, 224, 50, 58, 10, 73, 6, 36, 92, 194, 211,\n",
    "        172, 98, 145, 149, 228, 121, 231, 200, 55, 109, 141, 213, 78, 169, 108, 86, 244, 234, 101, 122, 174, 8, 186,\n",
    "        120, 37, 46, 28, 166, 180, 198, 232, 221, 116, 31, 75, 189, 139, 138, 112, 62, 181, 102, 72, 3, 246, 14, 97,\n",
    "        53, 87, 185, 134, 193, 29, 158, 225, 248, 152, 17, 105, 217, 142, 148, 155, 30, 135, 233, 206, 85, 40, 223, 140,\n",
    "        161, 137, 13, 191, 230, 66, 104, 65, 153, 45, 15, 176, 84, 187, 22]\n",
    "\n",
    "HW_byte = [0, 1, 1, 2, 1, 2, 2, 3, 1, 2, 2, 3, 2, 3, 3, 4, 1, 2, 2, 3, 2, 3, 3, 4, 2, 3, 3, 4, 3, 4, 4, 5, 1, 2, 2,\n",
    "            3, 2, 3, 3, 4, 2, 3, 3, 4, 3, 4, 4, 5, 2, 3, 3, 4, 3, 4, 4, 5, 3, 4, 4, 5, 4, 5, 5, 6, 1, 2, 2, 3, 2, 3,\n",
    "            3, 4, 2, 3, 3, 4, 3, 4, 4, 5, 2, 3, 3, 4, 3, 4, 4, 5, 3, 4, 4, 5, 4, 5, 5, 6, 2, 3, 3, 4, 3, 4, 4, 5, 3,\n",
    "            4, 4, 5, 4, 5, 5, 6, 3, 4, 4, 5, 4, 5, 5, 6, 4, 5, 5, 6, 5, 6, 6, 7, 1, 2, 2, 3, 2, 3, 3, 4, 2, 3, 3, 4,\n",
    "            3, 4, 4, 5, 2, 3, 3, 4, 3, 4, 4, 5, 3, 4, 4, 5, 4, 5, 5, 6, 2, 3, 3, 4, 3, 4, 4, 5, 3, 4, 4, 5, 4, 5, 5,\n",
    "            6, 3, 4, 4, 5, 4, 5, 5, 6, 4, 5, 5, 6, 5, 6, 6, 7, 2, 3, 3, 4, 3, 4, 4, 5, 3, 4, 4, 5, 4, 5, 5, 6, 3, 4,\n",
    "            4, 5, 4, 5, 5, 6, 4, 5, 5, 6, 5, 6, 6, 7, 3, 4, 4, 5, 4, 5, 5, 6, 4, 5, 5, 6, 5, 6, 6, 7, 4, 5, 5, 6, 5,\n",
    "            6, 6, 7, 5, 6, 6, 7, 6, 7, 7, 8]\n",
    "\n",
    "def train(epoch, model, freeze_BN=False):\n",
    "    \"\"\"\n",
    "    - epoch : the current epoch\n",
    "    - model : the current model  \n",
    "    - freeze_BN : whether to freeze batch normalization layers\n",
    "    \"\"\"\n",
    "    if freeze_BN:\n",
    "        model.eval()  # enter eval mode to freeze batch normalization layers\n",
    "    else:\n",
    "        model.train()  # enter training mode \n",
    "    # get the number of batches\n",
    "    num_iter = len(source_train_loader)\n",
    "    clf_criterion = nn.CrossEntropyLoss()\n",
    "    # train on each batch of data\n",
    "    for i, (source_data, source_label) in enumerate(source_train_loader, 1):\n",
    "        if cuda:\n",
    "            source_data, source_label = source_data.cuda(), source_label.cuda()\n",
    "        source_data, source_label = Variable(source_data), Variable(source_label)\n",
    "        optimizer.zero_grad()\n",
    "        source_preds = model(source_data)\n",
    "        preds = source_preds.data.max(1, keepdim=True)[1]\n",
    "        correct_batch = preds.eq(source_label.data.view_as(preds)).sum()\n",
    "        loss = clf_criterion(source_preds, source_label)\n",
    "        # optimize the cross-entropy loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i % log_interval == 0:\n",
    "            print('Train Epoch {}: [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tAcc: {:.6f}%'.format(\n",
    "                epoch, i * len(source_data), len(source_train_loader) * batch_size,\n",
    "                100. * i / len(source_train_loader), loss.data, 100. * correct_batch / len(source_data)))\n",
    "\n",
    "            \n",
    "### validation \n",
    "def validation(model):\n",
    "    # enter evaluation mode\n",
    "    model.eval()\n",
    "    valid_loss = 0\n",
    "    # the number of correct prediction\n",
    "    correct_valid = 0\n",
    "    clf_criterion = nn.CrossEntropyLoss()\n",
    "    for data, label in source_valid_loader:\n",
    "        if cuda:\n",
    "            data, label = data.cuda(), label.cuda()\n",
    "        data, label = Variable(data), Variable(label)\n",
    "        valid_preds = model(data)\n",
    "        # sum up batch loss\n",
    "        valid_loss += clf_criterion(valid_preds, label) \n",
    "        # get the index of the max probability\n",
    "        pred = valid_preds.data.max(1)[1] \n",
    "        # get the number of correct prediction\n",
    "        correct_valid += pred.eq(label.data.view_as(pred)).cpu().sum()\n",
    "    valid_loss /= len(source_valid_loader)\n",
    "    valid_acc = 100. * correct_valid / len(source_valid_loader.dataset)\n",
    "    print('Validation: loss: {:.4f}, accuracy: {}/{} ({:.6f}%)'.format(\n",
    "        valid_loss.data, correct_valid, len(source_valid_loader.dataset),\n",
    "        valid_acc))\n",
    "    return valid_loss, valid_acc\n",
    "\n",
    "### test/attack\n",
    "def test(model, device_id, disp_GE=True, model_flag='pretrained'):\n",
    "    \"\"\"\n",
    "    - model : the current model \n",
    "    - device_id : id of the tested device\n",
    "    - disp_GE : whether to attack/calculate guessing entropy (GE)\n",
    "    - model_flag : a string for naming GE result\n",
    "    \"\"\"\n",
    "    # enter evaluation mode\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    # the number of correct prediction\n",
    "    correct = 0\n",
    "    epoch = 0\n",
    "    clf_criterion = nn.CrossEntropyLoss()\n",
    "    if device_id == source_device_id: # attack on the source domain\n",
    "        test_num = source_test_num\n",
    "        test_loader = source_test_loader\n",
    "        real_key = real_key_01\n",
    "    else: # attack on the target domain\n",
    "        test_num = target_test_num\n",
    "        test_loader = target_test_loader\n",
    "        real_key = real_key_02\n",
    "    # Initialize the prediction and label lists(tensors)\n",
    "    predlist=torch.zeros(0,dtype=torch.long, device='cpu')\n",
    "    lbllist=torch.zeros(0,dtype=torch.long, device='cpu')\n",
    "    test_preds_all = torch.zeros((test_num, class_num), dtype=torch.float, device='cpu')\n",
    "    for data, label in test_loader:\n",
    "        if cuda:\n",
    "            data, label = data.cuda(), label.cuda()\n",
    "        data, label = Variable(data), Variable(label)\n",
    "        test_preds = model(data)\n",
    "        # sum up batch loss\n",
    "        test_loss += clf_criterion(test_preds, label) \n",
    "        # get the index of the max probability\n",
    "        pred = test_preds.data.max(1)[1]\n",
    "        # get the softmax results for attack/showing guessing entropy\n",
    "        softmax = nn.Softmax(dim=1)\n",
    "        test_preds_all[epoch*batch_size:(epoch+1)*batch_size, :] =softmax(test_preds)\n",
    "        # get the predictions (predlist) and real labels (lbllist) for showing confusion matrix\n",
    "        predlist=torch.cat([predlist,pred.view(-1).cpu()])\n",
    "        lbllist=torch.cat([lbllist,label.view(-1).cpu()])\n",
    "        # get the number of correct prediction\n",
    "        correct += pred.eq(label.data.view_as(pred)).cpu().sum()\n",
    "        epoch += 1\n",
    "    test_loss /= len(test_loader)\n",
    "    print('Target test loss: {:.4f}, Target test accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "        test_loss.data, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    # get the confusion matrix\n",
    "    confusion_mat = confusion_matrix(lbllist.numpy(), predlist.numpy())\n",
    "    # show the confusion matrix\n",
    "    plot_sonfusion_matrix(confusion_mat, classes = range(class_num))\n",
    "    # show the guessing entropy and success rate\n",
    "    if disp_GE:\n",
    "        plot_guessing_entropy(test_preds_all.numpy(), real_key, device_id, model_flag)\n",
    "\n",
    "\n",
    "### show the guessing entropy and success rate\n",
    "def plot_guessing_entropy(preds, real_key, device_id, model_flag):\n",
    "    \"\"\"\n",
    "    - preds : the probability for each class (n*256 for a byte, n*9 for Hamming weight)\n",
    "    - real_key : the key of the target device\n",
    "    - device_id : id of the target device\n",
    "    - model_flag : a string for naming GE result\n",
    "    \"\"\"\n",
    "    # GE/SR is averaged over 100 attacks \n",
    "    num_averaged = 100\n",
    "    # max trace num for attack\n",
    "    trace_num_max = 500\n",
    "    guessing_entropy = np.zeros((num_averaged, trace_num_max))\n",
    "    success_flag = np.zeros((num_averaged, trace_num_max))\n",
    "    if device_id == target_device_id: # attack on the target domain\n",
    "        plaintext = plaintexts_target\n",
    "    elif device_id == source_device_id: # attack on the source domain\n",
    "        plaintext = plaintexts_source\n",
    "    # attack multiples times for average\n",
    "    for time in range(num_averaged):\n",
    "        # select the attack traces randomly\n",
    "        random_index = list(range(plaintext.shape[0]))\n",
    "        random.shuffle(random_index)\n",
    "        random_index = random_index[0:trace_num_max]\n",
    "        # initialize score matrix\n",
    "        score_mat = np.zeros((trace_num_max, 256))\n",
    "        for key_guess in range(0, 256):\n",
    "            for i in range(0, trace_num_max):\n",
    "                initialState = plaintext[random_index[i]] ^ key_guess\n",
    "                sout = Sbox[initialState]\n",
    "                if labeling_method == 'identity':\n",
    "                    label = sout\n",
    "                elif labeling_method == 'hw':\n",
    "                    label = HW_byte[sout]\n",
    "                score_mat[i, key_guess] = preds[random_index[i], label]\n",
    "        score_mat = np.log(score_mat + 1e-40)\n",
    "        for i in range(0, trace_num_max):\n",
    "            log_likelihood = np.sum(score_mat[0:i+1,:], axis=0)\n",
    "            ranked = np.argsort(log_likelihood)[::-1]\n",
    "            guessing_entropy[time,i] =  list(ranked).index(real_key)\n",
    "            if list(ranked).index(real_key) == 0:\n",
    "                    success_flag[time, i] = 1\n",
    "    guessing_entropy = np.mean(guessing_entropy,axis=0)\n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    p1, = plt.plot(guessing_entropy[0:trace_num_max],color='red')\n",
    "    plt.xlabel('Number of trace')\n",
    "    plt.ylabel('Guessing entropy')\n",
    "    #np.save('./results/entropy_'+ labeling_method + '_{}_to_{}_'.format(source_device_id, device_id) + model_flag, guessing_entropy)\n",
    "    plt.subplot(1, 2, 2)       \n",
    "    success_flag = np.sum(success_flag, axis=0)\n",
    "    success_rate = success_flag/num_averaged \n",
    "    p2, = plt.plot(success_rate[0:trace_num_max], color='red')\n",
    "    plt.xlabel('Number of trace')\n",
    "    plt.ylabel('Success rate')\n",
    "    plt.show()\n",
    "    #np.save('./results/success_rate_' + labeling_method + '_{}_to_{}_'.format(source_device_id, device_id) + model_flag, success_rate)\n",
    "\n",
    "### show the confusion matrix \n",
    "def plot_sonfusion_matrix(cm, classes, normalize=False, title='Confusion matrix',cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    if normalize:\n",
    "        cm = cm.astype('float')/cm.sum(axis=1)[:,np.newaxis]\n",
    "    thresh = cm.max()/2.0\n",
    "    for i,j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j,i,cm[i,j], horizontalalignment='center',color='white' if cm[i,j] > thresh else 'black')   \n",
    "    plt.ylim((len(classes)-0.5, -0.5))\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predict label')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data complete!\n"
     ]
    }
   ],
   "source": [
    "source_device_id = 1\n",
    "target_device_id = 2\n",
    "real_key_01 = 0x01 # key of the source domain\n",
    "real_key_02 = 0x02 # key of the target domain\n",
    "lambda_ = 0.1 # Penalty coefficient\n",
    "labeling_method = 'hw' # labeling of trace\n",
    "preprocess = 'horizontal_standardization' # preprocess method\n",
    "batch_size = 250\n",
    "total_epoch = 100\n",
    "finetune_epoch = 15 # epoch number for fine-tuning\n",
    "lr = 0.001 # learning rate\n",
    "log_interval = 20 # epoch interval to log training information\n",
    "train_num = 20000\n",
    "valid_num = 5000\n",
    "source_test_num = 1000\n",
    "target_finetune_num = 500\n",
    "target_test_num = 9500\n",
    "trace_offset = 0\n",
    "trace_length = 1000\n",
    "source_file_path = './Data/ourdata//'\n",
    "target_file_path = './Data/device02/'\n",
    "no_cuda =False\n",
    "cuda = not no_cuda and torch.cuda.is_available()\n",
    "seed = 8\n",
    "torch.manual_seed(seed)\n",
    "if cuda:\n",
    "    torch.cuda.manual_seed(seed)\n",
    "if labeling_method == 'identity':\n",
    "    class_num = 256\n",
    "elif labeling_method == 'hw':\n",
    "    class_num = 9\n",
    "\n",
    "# to load traces and labels\n",
    "X_train_source = trace_array[0:25000]\n",
    "Y_train_source = textin_array[0:25000]\n",
    "X_attack_source = trace_array[25000:26000]\n",
    "Y_attack_source = textin_array[25000:26000]\n",
    "X_attack_target = np.load(target_file_path + 'X_attack.npy')\n",
    "Y_attack_target = np.load(target_file_path + 'Y_attack.npy')\n",
    "\n",
    "# to load plaintexts\n",
    "plaintexts_source = np.load(source_file_path + 'plaintexts_attack.npy')\n",
    "plaintexts_target = np.load(target_file_path + 'plaintexts_attack.npy')\n",
    "plaintexts_target = plaintexts_target[target_finetune_num:target_finetune_num+target_test_num]\n",
    "\n",
    "# preprocess of traces\n",
    "if preprocess == 'horizontal_standardization':\n",
    "    mn = np.repeat(np.mean(X_train_source, axis=1, keepdims=True), X_train_source.shape[1], axis=1)\n",
    "    std = np.repeat(np.std(X_train_source, axis=1, keepdims=True), X_train_source.shape[1], axis=1)\n",
    "    X_train_source = (X_train_source - mn)/std\n",
    "\n",
    "    mn = np.repeat(np.mean(X_attack_source, axis=1, keepdims=True), X_attack_source.shape[1], axis=1)\n",
    "    std = np.repeat(np.std(X_attack_source, axis=1, keepdims=True), X_attack_source.shape[1], axis=1)\n",
    "    X_attack_source = (X_attack_source - mn)/std\n",
    "    \n",
    "    mn = np.repeat(np.mean(X_attack_target, axis=1, keepdims=True), X_attack_target.shape[1], axis=1)\n",
    "    std = np.repeat(np.std(X_attack_target, axis=1, keepdims=True), X_attack_target.shape[1], axis=1)\n",
    "    X_attack_target = (X_attack_target - mn)/std  \n",
    "elif preprocess == 'horizontal_scaling':\n",
    "    scaler = preprocessing.MinMaxScaler(feature_range=(-1, 1)).fit(X_train_source.T)\n",
    "    X_train_source = scaler.transform(X_train_source.T).T\n",
    "\n",
    "    scaler = preprocessing.MinMaxScaler(feature_range=(-1, 1)).fit(X_attack_source.T)\n",
    "    X_attack_source = scaler.transform(X_attack_source.T).T\n",
    "    \n",
    "    scaler = preprocessing.MinMaxScaler(feature_range=(-1, 1)).fit(X_attack_target.T)\n",
    "    X_attack_target = scaler.transform(X_attack_target.T).T\n",
    "\n",
    "# parameters of data loader\n",
    "kwargs_source_train = {\n",
    "        'trs_file': X_train_source[0:train_num,:],\n",
    "        'label_file': Y_train_source[0:train_num],\n",
    "        'trace_num':train_num,\n",
    "        'trace_offset':trace_offset,\n",
    "        'trace_length':trace_length,\n",
    "}\n",
    "kwargs_source_valid = {\n",
    "        'trs_file': X_train_source[train_num:train_num+valid_num,:],\n",
    "        'label_file': Y_train_source[train_num:train_num+valid_num],\n",
    "        'trace_num':valid_num,\n",
    "        'trace_offset':trace_offset,\n",
    "        'trace_length':trace_length,\n",
    "}\n",
    "kwargs_source_test = {\n",
    "        'trs_file': X_attack_source,\n",
    "        'label_file': Y_attack_source,\n",
    "        'trace_num':source_test_num,\n",
    "        'trace_offset':trace_offset,\n",
    "        'trace_length':trace_length,\n",
    "}\n",
    "kwargs_target_finetune = {\n",
    "        'trs_file': X_attack_target[0:target_finetune_num,:],\n",
    "        'label_file': Y_attack_target[0:target_finetune_num],\n",
    "        'trace_num':target_finetune_num,\n",
    "        'trace_offset':trace_offset,\n",
    "        'trace_length':trace_length,\n",
    "}\n",
    "kwargs_target = {\n",
    "        'trs_file': X_attack_target[target_finetune_num:target_finetune_num+target_test_num, :],\n",
    "        'label_file': Y_attack_target[target_finetune_num:target_finetune_num+target_test_num],\n",
    "        'trace_num':target_test_num,\n",
    "        'trace_offset':trace_offset,\n",
    "        'trace_length':trace_length,\n",
    "}\n",
    "source_train_loader = load_training(batch_size, kwargs_source_train)\n",
    "source_valid_loader = load_training(batch_size, kwargs_source_valid)\n",
    "source_test_loader = load_testing(batch_size, kwargs_source_test)\n",
    "target_finetune_loader = load_training(batch_size, kwargs_target_finetune)\n",
    "target_test_loader = load_testing(batch_size, kwargs_target)\n",
    "print('Load data complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### the pre-trained model\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, num_classes=class_num):\n",
    "        super(Net, self).__init__()\n",
    "        # the encoder part\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv1d(1, 16, kernel_size=1),\n",
    "            nn.SELU(),\n",
    "            nn.AvgPool1d(kernel_size=2, stride=2),\n",
    "            nn.Conv1d(16, 32, kernel_size=50),\n",
    "            nn.SELU(),\n",
    "            nn.AvgPool1d(kernel_size=50, stride=50),\n",
    "            nn.Conv1d(32, 64, kernel_size=3),\n",
    "            nn.SELU(),\n",
    "            nn.AvgPool1d(kernel_size=2, stride=2),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        # the fully-connected layer 1\n",
    "        self.classifier_1 = nn.Sequential(\n",
    "            nn.Linear(384, 20),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        # the output layer\n",
    "        self.final_classifier = nn.Sequential(\n",
    "            nn.Linear(20, num_classes)\n",
    "        )\n",
    "\n",
    "    # how the network runs\n",
    "    def forward(self, input):\n",
    "        x = self.features(input)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier_1(x)\n",
    "        output = self.final_classifier(x)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Construct model complete\n"
     ]
    }
   ],
   "source": [
    "# create a network\n",
    "model = Net(num_classes=class_num)\n",
    "print('Construct model complete')\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (features): Sequential(\n",
      "    (0): Conv1d(1, 16, kernel_size=(1,), stride=(1,))\n",
      "    (1): SELU()\n",
      "    (2): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
      "    (3): Conv1d(16, 32, kernel_size=(50,), stride=(1,))\n",
      "    (4): SELU()\n",
      "    (5): AvgPool1d(kernel_size=(50,), stride=(50,), padding=(0,))\n",
      "    (6): Conv1d(32, 64, kernel_size=(3,), stride=(1,))\n",
      "    (7): SELU()\n",
      "    (8): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
      "    (9): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      "  (classifier_1): Sequential(\n",
      "    (0): Linear(in_features=384, out_features=20, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "  )\n",
      "  (final_classifier): Sequential(\n",
      "    (0): Linear(in_features=20, out_features=9, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Construct model complete\n",
      "Train Epoch 1:\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (250x192 and 384x20)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, train_epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrain Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 20\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 41\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(epoch, model, freeze_BN)\u001b[0m\n\u001b[1;32m     39\u001b[0m source_data, source_label \u001b[38;5;241m=\u001b[39m Variable(source_data), Variable(source_label)\n\u001b[1;32m     40\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 41\u001b[0m source_preds \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m preds \u001b[38;5;241m=\u001b[39m source_preds\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mmax(\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     43\u001b[0m correct_batch \u001b[38;5;241m=\u001b[39m preds\u001b[38;5;241m.\u001b[39meq(source_label\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mview_as(preds))\u001b[38;5;241m.\u001b[39msum()\n",
      "File \u001b[0;32m~/anaconda3/envs/gpu/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[8], line 32\u001b[0m, in \u001b[0;36mNet.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     30\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m     31\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 32\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclassifier_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_classifier(x)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/anaconda3/envs/gpu/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/gpu/lib/python3.9/site-packages/torch/nn/modules/container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/gpu/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/gpu/lib/python3.9/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (250x192 and 384x20)"
     ]
    }
   ],
   "source": [
    "\n",
    "          \n",
    "          \n",
    "# create a network\n",
    "print('Construct model complete')\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "# initialize a big enough loss\n",
    "min_loss = 1000\n",
    "# load the pre-trained model\n",
    "#checkpoint = torch.load('./models/new{}.pth'.format(source_device_id))\n",
    "optimizer = optim.Adam([\n",
    "        {'params': model.features.parameters()},\n",
    "        {'params': model.classifier_1.parameters()},\n",
    "        {'params': model.final_classifier.parameters()}\n",
    "    ], lr=lr)\n",
    "# restore the optimizer state\n",
    "#optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "train_epoch=150\n",
    "# Usage:\n",
    "for epoch in range(1, train_epoch + 1):\n",
    "    print(f'Train Epoch {epoch}:')\n",
    "    train(epoch, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    \n",
    "    print('Result on source device:')\n",
    "    test(model, source_device_id, model_flag='finetuned_source')\n",
    "    \n",
    "    print('Result on target device:')\n",
    "    test(model, target_device_id, model_flag='finetuned_target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
