{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "534df921",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-01 12:07:28.485002: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "from math import sqrt, isnan\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.signal\n",
    "import scipy.fftpack\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from tqdm import tnrange\n",
    "from tensorflow.keras.utils import to_categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae7c9c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pdb\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from sklearn import preprocessing\n",
    "import random\n",
    "\n",
    "\n",
    "def preprocess_data(x_data, method):\n",
    "    # preprocess of traces\n",
    "    if method == 'norm':     # 'horizontal_standardization':\n",
    "        print('[LOG] -- using {} method to preprocessing the data.'.format(method))\n",
    "        mn = np.repeat(np.mean(x_data, axis=1, keepdims=True), x_data.shape[1], axis=1)\n",
    "        std = np.repeat(np.std(x_data, axis=1, keepdims=True), x_data.shape[1], axis=1)\n",
    "        x_data = (x_data - mn)/std\n",
    "    elif method == 'scaling':    #  'horizontal_scaling':\n",
    "        print('[LOG] -- using {} method to preprocessing the data.'.format(method))\n",
    "        scaler = preprocessing.MinMaxScaler(feature_range=(-1, 1)).fit(x_data.T)\n",
    "        x_data = scaler.transform(x_data.T).T\n",
    "    else:\n",
    "        print('[LOG] -- not perform preprocessing method to the data.')\n",
    "\n",
    "    return x_data\n",
    "\n",
    "\n",
    "sbox = [\n",
    "    # 0    1    2    3    4    5    6    7    8    9    a    b    c    d    e    f\n",
    "    0x63, 0x7c, 0x77, 0x7b, 0xf2, 0x6b, 0x6f, 0xc5, 0x30, 0x01, 0x67, 0x2b, 0xfe, 0xd7, 0xab, 0x76,  # 0\n",
    "    0xca, 0x82, 0xc9, 0x7d, 0xfa, 0x59, 0x47, 0xf0, 0xad, 0xd4, 0xa2, 0xaf, 0x9c, 0xa4, 0x72, 0xc0,  # 1\n",
    "    0xb7, 0xfd, 0x93, 0x26, 0x36, 0x3f, 0xf7, 0xcc, 0x34, 0xa5, 0xe5, 0xf1, 0x71, 0xd8, 0x31, 0x15,  # 2\n",
    "    0x04, 0xc7, 0x23, 0xc3, 0x18, 0x96, 0x05, 0x9a, 0x07, 0x12, 0x80, 0xe2, 0xeb, 0x27, 0xb2, 0x75,  # 3\n",
    "    0x09, 0x83, 0x2c, 0x1a, 0x1b, 0x6e, 0x5a, 0xa0, 0x52, 0x3b, 0xd6, 0xb3, 0x29, 0xe3, 0x2f, 0x84,  # 4\n",
    "    0x53, 0xd1, 0x00, 0xed, 0x20, 0xfc, 0xb1, 0x5b, 0x6a, 0xcb, 0xbe, 0x39, 0x4a, 0x4c, 0x58, 0xcf,  # 5\n",
    "    0xd0, 0xef, 0xaa, 0xfb, 0x43, 0x4d, 0x33, 0x85, 0x45, 0xf9, 0x02, 0x7f, 0x50, 0x3c, 0x9f, 0xa8,  # 6\n",
    "    0x51, 0xa3, 0x40, 0x8f, 0x92, 0x9d, 0x38, 0xf5, 0xbc, 0xb6, 0xda, 0x21, 0x10, 0xff, 0xf3, 0xd2,  # 7\n",
    "    0xcd, 0x0c, 0x13, 0xec, 0x5f, 0x97, 0x44, 0x17, 0xc4, 0xa7, 0x7e, 0x3d, 0x64, 0x5d, 0x19, 0x73,  # 8\n",
    "    0x60, 0x81, 0x4f, 0xdc, 0x22, 0x2a, 0x90, 0x88, 0x46, 0xee, 0xb8, 0x14, 0xde, 0x5e, 0x0b, 0xdb,  # 9\n",
    "    0xe0, 0x32, 0x3a, 0x0a, 0x49, 0x06, 0x24, 0x5c, 0xc2, 0xd3, 0xac, 0x62, 0x91, 0x95, 0xe4, 0x79,  # a\n",
    "    0xe7, 0xc8, 0x37, 0x6d, 0x8d, 0xd5, 0x4e, 0xa9, 0x6c, 0x56, 0xf4, 0xea, 0x65, 0x7a, 0xae, 0x08,  # b\n",
    "    0xba, 0x78, 0x25, 0x2e, 0x1c, 0xa6, 0xb4, 0xc6, 0xe8, 0xdd, 0x74, 0x1f, 0x4b, 0xbd, 0x8b, 0x8a,  # c\n",
    "    0x70, 0x3e, 0xb5, 0x66, 0x48, 0x03, 0xf6, 0x0e, 0x61, 0x35, 0x57, 0xb9, 0x86, 0xc1, 0x1d, 0x9e,  # d\n",
    "    0xe1, 0xf8, 0x98, 0x11, 0x69, 0xd9, 0x8e, 0x94, 0x9b, 0x1e, 0x87, 0xe9, 0xce, 0x55, 0x28, 0xdf,  # e\n",
    "    0x8c, 0xa1, 0x89, 0x0d, 0xbf, 0xe6, 0x42, 0x68, 0x41, 0x99, 0x2d, 0x0f, 0xb0, 0x54, 0xbb, 0x16  # f\n",
    "]\n",
    "\n",
    "\n",
    "def calc_hamming_weight(n):\n",
    "    return bin(n).count(\"1\")\n",
    "\n",
    "\n",
    "def get_HW():\n",
    "    HW = []\n",
    "    for i in range(0, 256):\n",
    "        hw_val = calc_hamming_weight(i)\n",
    "        HW.append(hw_val)\n",
    "    return HW\n",
    "\n",
    "\n",
    "HW = get_HW()\n",
    "\n",
    "\n",
    "def aes_internal(inp_data_byte, key_byte):\n",
    "    inp_data_byte = int(inp_data_byte)\n",
    "    return sbox[inp_data_byte ^ key_byte]\n",
    "\n",
    "\n",
    "def create_hw_label_mapping():\n",
    "    ''' this function return a mapping that maps hw label to number per class '''\n",
    "    HW = defaultdict(list)\n",
    "    for i in range(0, 256):\n",
    "        hw_val = calc_hamming_weight(i)\n",
    "        HW[hw_val].append(i)\n",
    "    return HW\n",
    "\n",
    "\n",
    "def get_one_label(text_i, target_byte, key_byte, leakage_model):\n",
    "    ''''''\n",
    "    label = aes_internal(text_i[target_byte], key_byte)\n",
    "    if 'HW' == leakage_model:\n",
    "        label = HW[label]\n",
    "    return label\n",
    "\n",
    "\n",
    "def get_labels(plain_text, key_byte, target_byte, leakage_model):\n",
    "    ''' get labels for a batch of data '''\n",
    "    labels = []\n",
    "    for i in range(plain_text.shape[0]):\n",
    "        text_i = plain_text[i]\n",
    "        label = get_one_label(text_i, target_byte, key_byte, leakage_model)\n",
    "        labels.append(label)\n",
    "\n",
    "    if 'HW' == leakage_model:\n",
    "        try:\n",
    "            assert(set(labels) == set(list(range(9))))\n",
    "        except Exception:\n",
    "            print('[LOG] -- not all class have data: ', set(labels))\n",
    "    else:\n",
    "        try:\n",
    "            assert(set(labels) == set(range(256)))\n",
    "        except Exception:\n",
    "            print('[LOG] -- not all class have data: ', set(labels))\n",
    "    labels = np.array(labels)\n",
    "    return labels\n",
    "\n",
    "\n",
    "def shift_the_data(shifted, attack_window, trace_mat, textin_mat):\n",
    "    start_idx, end_idx = attack_window[0], attack_window[1]\n",
    "\n",
    "    if shifted:\n",
    "        print('[LOG] -- data will be shifted in range: ', [0, shifted])\n",
    "        shifted_traces = []\n",
    "        for i in range(textin_mat.shape[0]):\n",
    "            random_int = random.randint(0, shifted)\n",
    "            trace_i = trace_mat[i, start_idx+random_int:end_idx+random_int]\n",
    "            shifted_traces.append(trace_i)\n",
    "        trace_mat = np.array(shifted_traces)\n",
    "    else:\n",
    "        print('[LOG] -- no random delay apply to the data')\n",
    "        trace_mat = trace_mat[:, start_idx:end_idx]\n",
    "\n",
    "    return trace_mat, textin_mat\n",
    "\n",
    "\n",
    "def unpack_data(whole_pack):\n",
    "    try:\n",
    "        traces, plain_text, key = whole_pack['power_trace'], whole_pack['plain_text'], whole_pack['key']\n",
    "    except KeyError:\n",
    "        try:\n",
    "            traces, plain_text, key = whole_pack['power_trace'], whole_pack['plaintext'], whole_pack['key']\n",
    "        except KeyError:\n",
    "            traces, plain_text, key = whole_pack['trace_mat'], whole_pack['textin_mat'], whole_pack['key']\n",
    "    return traces, plain_text, key\n",
    "\n",
    "\n",
    "def load_data_base(whole_pack, attack_window, method, trace_num=0, shifted=0):\n",
    "    if isinstance(attack_window, str):\n",
    "        tmp = attack_window.split('_')\n",
    "        attack_window = [int(tmp[0]), int(tmp[1])]\n",
    "\n",
    "    traces, plain_text, key = unpack_data(whole_pack)\n",
    "\n",
    "    if trace_num:\n",
    "        traces = traces[:trace_num, :]\n",
    "        plain_text = plain_text[:trace_num, :]\n",
    "\n",
    "    traces, plain_text = shift_the_data(shifted, attack_window, traces, plain_text)\n",
    "\n",
    "    if method:\n",
    "        traces = preprocess_data(traces, method)\n",
    "    return traces, plain_text, key\n",
    "\n",
    "\n",
    "def load_data_base_test(whole_pack, attack_window, method, trace_num=0, shifted=0):\n",
    "    if isinstance(attack_window, str):\n",
    "        tmp = attack_window.split('_')\n",
    "        attack_window = [int(tmp[0]), int(tmp[1])]\n",
    "\n",
    "    traces, plain_text, key = unpack_data(whole_pack)\n",
    "\n",
    "    if trace_num:\n",
    "        traces = traces[-trace_num:, :]\n",
    "        plain_text = plain_text[-trace_num:, :]\n",
    "\n",
    "    traces, plain_text = shift_the_data(shifted, attack_window, traces, plain_text)\n",
    "\n",
    "    if method:\n",
    "        traces = preprocess_data(traces, method)\n",
    "    return traces, plain_text, key\n",
    "\n",
    "\n",
    "def data_info(power_traces_shape, plain_text_shape, key):\n",
    "    print('shape of the plain text matrix : ', plain_text_shape)\n",
    "    print('shape of the power trace matrix: ', power_traces_shape)\n",
    "    print('Encryption key: ', key)\n",
    "    print('-' * 90)\n",
    "\n",
    "\n",
    "def sanity_check(input_layer_shape, X_profiling):\n",
    "    if input_layer_shape[1] != X_profiling.shape[1]:\n",
    "        print(\"Error: model input shape %d instead of %d is not expected ...\" % (input_layer_shape[1], len(X_profiling[0])))\n",
    "        sys.exit(-1)\n",
    "    # Adapt the data shape according our model input\n",
    "    if len(input_layer_shape) == 2:\n",
    "        # This is a MLP\n",
    "        Reshaped_X_profiling = X_profiling\n",
    "    elif len(input_layer_shape) == 3:\n",
    "        # This is a CNN: expand the dimensions\n",
    "        Reshaped_X_profiling = X_profiling.reshape((X_profiling.shape[0], X_profiling.shape[1], 1))\n",
    "    else:\n",
    "        print(\"Error: model input shape length %d is not expected ...\" % len(input_layer_shape))\n",
    "        sys.exit(-1)\n",
    "    return Reshaped_X_profiling\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4c62c4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_training_data():\n",
    "    '''data loading function'''\n",
    "    target_byte = 2\n",
    "    leakage_model = 'ID'\n",
    "    data_path = '/home/mabon/Cross_EM/datasets/stm_em/T1/S1_K1_150k_L11.npz'\n",
    "    trace_num = 150000\n",
    "    method = 0\n",
    "    attack_window = '1200_2200'\n",
    "\n",
    "    whole_pack = np.load(data_path)\n",
    "    traces, text_in, key = load_data_base(whole_pack, attack_window, method, trace_num=trace_num, shifted=0)\n",
    "    \n",
    "    '''\n",
    "    #the following code is for downsampling for training\n",
    "    n1=[]\n",
    "    for i in range(0,int((np.shape(traces)[0]))):\n",
    "        t1=[]\n",
    "        for j in range(0,4000,4):\n",
    "            t1.append(traces[i][j])\n",
    "        n1.append(t1)\n",
    "    traces=np.array(n1)\n",
    "    '''\n",
    "\n",
    "\n",
    "    labels = get_labels(text_in, key[target_byte], target_byte, leakage_model)\n",
    "\n",
    "    inp_shape = (traces.shape[1], 1)\n",
    "    data_info(traces.shape, text_in.shape, key)\n",
    "\n",
    "    clsNum = 9 if 'HW' == leakage_model else 256\n",
    "    print('[LOG] -- class number is: ', clsNum)\n",
    "    labels = to_categorical(labels, clsNum)\n",
    "\n",
    "    return traces, labels, inp_shape, clsNum,text_in,key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fc03f038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG] -- no random delay apply to the data\n",
      "shape of the plain text matrix :  (150000, 16)\n",
      "shape of the power trace matrix:  (150000, 1000)\n",
      "Encryption key:  [ 43 126  21  22  40 174 210 166 171 247  21 136   9 207  79  60]\n",
      "------------------------------------------------------------------------------------------\n",
      "[LOG] -- class number is:  256\n"
     ]
    }
   ],
   "source": [
    "traces, labels, inp_shape, clsNum,text_in,key=load_training_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "98907fc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150000, 16)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(text_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "149e7243",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150000, 1000)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(traces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4b38bab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6e54656c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "def lda(traces, labels, n_components):\n",
    "    \"\"\"Performs LDA on side-channel traces.\n",
    "\n",
    "    Args:\n",
    "    traces: A 2D NumPy array containing the side-channel traces.\n",
    "    labels: A 2D NumPy array containing the labels for the traces.\n",
    "    n_components: The number of LDA components to use.\n",
    "\n",
    "    Returns:\n",
    "    A 2D NumPy array containing the LDA-transformed traces.\n",
    "    \"\"\"\n",
    "\n",
    "    lda = LinearDiscriminantAnalysis(n_components=n_components)\n",
    "    lda.fit(traces, labels)\n",
    "    lda_traces = lda.transform(traces)\n",
    "    return lda_traces\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bcfefd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = labels.argmax(axis=1)\n",
    "\n",
    "n_classes = 255\n",
    "lda_traces = lda(traces, labels, n_classes)\n",
    "\n",
    "# Save the LDA-transformed traces:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "698a1003",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150000, 255)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(lda_traces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "987b3234",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('/home/mabon/LDA/DataSets/stm/T1/S1_K1_150k_L11.npz',power_trace=lda_traces,plain_text=text_in,key=key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f76db65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow] *",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
